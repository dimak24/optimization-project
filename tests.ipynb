{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "from model import LogisticInstance\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import time\n",
    "\n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(method_name, method, real_f_name, real_f, **kwargs):\n",
    "    t = time.monotonic()\n",
    "    x_opt = method(**kwargs)\n",
    "    print('{}:\\n  x_opt = {}\\n  {} = {}\\n  time = {}s\\n'.format(method_name, x_opt,\n",
    "                                                                real_f_name, real_f(x_opt),\n",
    "                                                                time.monotonic() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple 1D case\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{e^{x-2}(5-x)}{1 + e^{x-2}} &\\to max \\\\\n",
    "x & \\le 2\n",
    "\\end{align*}\n",
    "\n",
    "Theoretical answer is $x_{opt} \\sim 2.44$ in unconditional case and $x_{opt} = 2$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.])\n",
    "b = -2.\n",
    "c = np.array([-1.])\n",
    "d = 5.\n",
    "F = np.array([[1.]])\n",
    "g = np.array([2.])\n",
    "\n",
    "issue = LogisticInstance(a, b, c, d, F, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent, conv. by argument, h_k = 0.1:\n",
      "  x_opt = [2.44261703]\n",
      "  expected_profit = 1.5571455818420734\n",
      "  time = 0.011573543000849895s\n",
      "\n",
      "Gradient descent, conv. by argument, h_k = 1/k:\n",
      "  x_opt = [2.34713717]\n",
      "  expected_profit = 1.554373820931351\n",
      "  time = 0.1462936419993639s\n",
      "\n",
      "Gradient descent, conv. by argument, h_k = 1/k:\n",
      "  x_opt = [2.24594916]\n",
      "  expected_profit = 1.545516050628967\n",
      "  time = 0.018999893000000156s\n",
      "\n",
      "Gradient descent, conv. by argument, h_k = 1/sqrt(k):\n",
      "  x_opt = [2.44255797]\n",
      "  expected_profit = 1.5571455722441265\n",
      "  time = 0.004981541002052836s\n",
      "\n",
      "\n",
      "\n",
      "Accelerated Nesterov gradient descent, h_k = 0.5:\n",
      "  x_opt = [2.4427758]\n",
      "  expected_profit = 1.5571455971164652\n",
      "  time = 0.0015145889992709272s\n",
      "\n",
      "Accelerated Nesterov gradient descent, h_k = 1/sqrt(k):\n",
      "  x_opt = [2.44126018]\n",
      "  expected_profit = 1.5571448252478892\n",
      "  time = 0.0018778400008159224s\n",
      "\n",
      "\n",
      "\n",
      "Newton method:\n",
      "  x_opt = [2.44285568]\n",
      "  expected_profit = 1.5571455989971115\n",
      "  time = 0.0006504790035251062s\n",
      "\n",
      "Broyden-Fletcher-Goldfarb-Shanno method (quasy-Newton):\n",
      "  x_opt = [2.4428544]\n",
      "  expected_profit = 1.557145598997611\n",
      "  time = 0.0004517460001807194s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unconditional optimization\n",
    "\n",
    "f = lambda x: -issue.log_expected_profit(x)\n",
    "df = lambda x: -issue.dlog_expected_profit(x)\n",
    "d2f = lambda x: -issue.d2log_expected_profit(x)\n",
    "\n",
    "\n",
    "\n",
    "test('Gradient descent, conv. by argument, h_k = 0.1', \n",
    "     GradientDescent(h=0.1), \n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "test('Gradient descent, conv. by argument, h_k = 1/k', \n",
    "     GradientDescent(h=lambda step: 1. / step), \n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "test('Gradient descent, conv. by argument, h_k = 1/k', \n",
    "     GradientDescent(h=lambda step: 1. / step,\n",
    "                     convergence_condition=Convergence.ByValue), \n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "test('Gradient descent, conv. by argument, h_k = 1/sqrt(k)', \n",
    "     GradientDescent(h=lambda step: step ** -0.5), \n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test('Accelerated Nesterov gradient descent, h_k = 0.5', \n",
    "     AcceleratedNesterovGradientDescent(),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "test('Accelerated Nesterov gradient descent, h_k = 1/sqrt(k)', \n",
    "     AcceleratedNesterovGradientDescent(h=lambda step: step ** -0.5),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test('Newton method',\n",
    "     NewtonMethod(),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.array([1.]), df=df, d2f=d2f)\n",
    "\n",
    "test('Broyden-Fletcher-Goldfarb-Shanno method (quasy-Newton)', \n",
    "     BroydenFletcherGoldfarbShannoMethod(),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.array([1.]), df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty method, pen_k = [1] * (k-1), ext_pen_func = max(0, x)^0.7,\n",
      "unconditional method -- gradient descent with h_k=0.01:\n",
      "  x_opt = [2.00233491]\n",
      "  expected_profit = 1.500582363040146\n",
      "  time = 0.06820202099697781s\n",
      "\n",
      "Conditional gradient method, k = 2 / (3 + step):\n",
      "  x_opt = [1.99783784]\n",
      "  expected_profit = 1.4994582913553598\n",
      "  time = 0.039937362002092414s\n",
      "\n",
      "Conditional gradient method, k = 1 / sqrt(step):\n",
      "  x_opt = [2.]\n",
      "  expected_profit = 1.5\n",
      "  time = 0.0006944110027689021s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conditional\n",
    "\n",
    "g = lambda x: np.dot(issue.F , x) -issue.g\n",
    "dg = lambda x: issue.F\n",
    "\n",
    "test('''Penalty method, pen_k = [1] * (k-1), ext_pen_func = max(0, x)^0.7,\n",
    "unconditional method -- gradient descent with h_k=0.01''', \n",
    "     PenaltyMethod(),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df,\n",
    "     g=g, dg=dg)\n",
    "\n",
    "test('Conditional gradient method, k = 2 / (3 + step)',\n",
    "     ConditionalGradientMethod(),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df,\n",
    "     g=g)\n",
    "\n",
    "test('Conditional gradient method, k = 1 / sqrt(step)',\n",
    "     ConditionalGradientMethod(k=lambda step: 1. / np.sqrt(step)),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.zeros(issue.a.shape[0]), df=df,\n",
    "     g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x_opt =  [2.] \n",
      "   prob =  0.5\n"
     ]
    }
   ],
   "source": [
    "# linear\n",
    "\n",
    "A = matrix(-issue.a)\n",
    "B = matrix([issue.b])\n",
    "F = matrix(issue.F)\n",
    "G = matrix(issue.g)\n",
    "\n",
    "\n",
    "sol = solvers.lp(A, F, G)\n",
    "print('  x_opt = ', np.array(sol['x']).T[0], '\\n   prob = ', issue.prob(np.array(sol['x']).T[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex 2D case\n",
    "\n",
    "x = (price, amount of advertising, quality of packing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([-4, 2, 1])\n",
    "b = 0.\n",
    "c = np.array([4000., -40., -4.])\n",
    "d = -10.\n",
    "F = np.array([-c, np.array([0., -1., 0.]), np.array([0., 0., -1.])])\n",
    "g = np.array([d - 100., 0., 0.])\n",
    "\n",
    "issue = LogisticInstance(a, b, c, d, F, g)\n",
    "\n",
    "f = lambda x: -issue.log_expected_profit(x)\n",
    "df = lambda x: -issue.dlog_expected_profit(x)\n",
    "d2f = lambda x: -issue.d2log_expected_profit(x)\n",
    "g = lambda x: np.dot(issue.F , x) -issue.g\n",
    "dg = lambda x: issue.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional gradient:\n",
      "  x_opt = [  2.19556097 190.75708493 190.72844573]\n",
      "  expected_profit = 379.04670443815667\n",
      "  time = 0.06694302299729316s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test('''Conditional gradient''', \n",
    "     ConditionalGradientMethod(precision=1e-3),\n",
    "     'expected_profit', issue.expected_profit,\n",
    "     f=f, x0=np.array([1, 1, 1]), df=df,\n",
    "     g=g, dg=dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
